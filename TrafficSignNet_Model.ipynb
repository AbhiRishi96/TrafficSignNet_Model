{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary to map class IDs to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {\n",
    "    0: 'Speed limit (20km/h)', 1: 'Speed limit (30km/h)', 2: 'Speed limit (50km/h)', 3: 'Speed limit (60km/h)',\n",
    "    4: 'Speed limit (70km/h)', 5: 'Speed limit (80km/h)', 6: 'End of speed limit (80km/h)', 7: 'Speed limit (100km/h)',\n",
    "    8: 'Speed limit (120km/h)', 9: 'No passing', 10: 'No passing for vehicles over 3.5 metric tons',\n",
    "    11: 'Right-of-way at the next intersection', 12: 'Priority road', 13: 'Yield', 14: 'Stop', 15: 'No vehicles',\n",
    "    16: 'Vehicles over 3.5 metric tons prohibited', 17: 'No entry', 18: 'General caution', 19: 'Dangerous curve to the left',\n",
    "    20: 'Dangerous curve to the right', 21: 'Double curve', 22: 'Bumpy road', 23: 'Slippery road',\n",
    "    24: 'Road narrows on the right', 25: 'Road work', 26: 'Traffic signals', 27: 'Pedestrians', 28: 'Children crossing',\n",
    "    29: 'Bicycles crossing', 30: 'Beware of ice/snow', 31: 'Wild animals crossing', 32: 'End of all speed and passing limits',\n",
    "    33: 'Turn right ahead', 34: 'Turn left ahead', 35: 'Ahead only', 36: 'Go straight or right', 37: 'Go straight or left',\n",
    "    38: 'Keep right', 39: 'Keep left', 40: 'Roundabout mandatory', 41: 'End of no passing',\n",
    "    42: 'End of no passing by vehicles over 3.5 metric tons'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Traffic Sign Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, meta_csv, meta_images_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.meta_data = pd.read_csv(meta_csv)\n",
    "        self.meta_images_dir = meta_images_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 7]\n",
    "        image = cv2.imread(img_name)\n",
    "        print(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = int(self.data.iloc[idx, 6])\n",
    "        \n",
    "        row = self.meta_data.iloc[label]  \n",
    "        shape_id = int(row['ShapeId'])\n",
    "        color_id = int(row['ColorId'])\n",
    "        \n",
    "        sign_id = None  \n",
    "        sign_id_str = str(row['SignId']).strip()\n",
    "        if sign_id_str and sign_id_str != 'None':\n",
    "            sign_id = float(sign_id_str.split('.')[0])\n",
    "        else:\n",
    "            sign_id = -1  \n",
    "        \n",
    "        meta_img_name = row['Path']\n",
    "        meta_image = cv2.imread(meta_img_name)\n",
    "        meta_image = cv2.cvtColor(meta_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            meta_image = self.transform(meta_image)\n",
    "        \n",
    "        return image, label, shape_id, color_id, sign_id, meta_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define TrafficSignNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=43, labels=None):\n",
    "        super(TrafficSignNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.labels = labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    @torch.jit.export\n",
    "    def get_class_label(self, class_id):\n",
    "        return self.labels[class_id] if self.labels else \"Unknown\"\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(TrafficSignNet(num_classes=43, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset, device):\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, labels, shape_ids, color_ids, sign_ids, meta_images in loader:\n",
    "        images = images.to(device)\n",
    "        mean += images.mean([0, 2, 3])\n",
    "        std += images.std([0, 2, 3])\n",
    "    mean /= len(loader)\n",
    "    std /= len(loader)\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, shape_ids, color_ids, sign_ids, meta_images in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `train_model` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=15, early_stopping_patience=5):\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    best_test_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels, shape_ids, color_ids, sign_ids, meta_images in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        test_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "        test_loss_history.append(test_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        # Save the Model Checkpoint\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            no_improvement_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_traffic_sign_model.ckpt')\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # Plot Loss Curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(test_loss_history, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `test_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, shape_ids, color_ids, sign_ids, meta_images in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torchscript(model, device, filename='traffic_sign_model.pt'):\n",
    "    model.eval()\n",
    "    example_input = torch.rand(1, 3, 32, 32).to(device)\n",
    "    traced_script_module = torch.jit.trace(model, example_input)\n",
    "    traced_script_module.save(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets & Defining Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths\n",
    "train_csv = 'csvs/Train.csv'\n",
    "test_csv = 'csvs/Test.csv'\n",
    "meta_csv = 'csvs/Meta.csv'\n",
    "train_images_dir = 'Datasets/Train'\n",
    "test_images_dir = 'Datasets/Test'\n",
    "meta_images_dir = 'Datasets/Meta'\n",
    "\n",
    "#Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "train_dataset = TrafficSignDataset(csv_file=train_csv, root_dir=train_images_dir, meta_csv=meta_csv, meta_images_dir=meta_images_dir, transform=transform)\n",
    "test_dataset = TrafficSignDataset(csv_file=test_csv, root_dir=test_images_dir, meta_csv=meta_csv, meta_images_dir=meta_images_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define Device and Instantiate Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TrafficSignNet(num_classes=43, labels=labels).to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute train & test scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing training script\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=15, early_stopping_patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.41%\n"
     ]
    }
   ],
   "source": [
    "# executing test script\n",
    "test_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_predictions(model, test_loader, device, labels_dict, num_samples=10):\n",
    "    sample_images, sample_labels, shape_ids, color_ids, sign_ids, meta_images = next(iter(test_loader))\n",
    "    sample_images = sample_images.to(device)\n",
    "    outputs = model(sample_images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    sample_labels = sample_labels.cpu().numpy()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        print(f'Predicted: {labels_dict[predicted[i]]}, Actual: {labels_dict[sample_labels[i]]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Vehicles over 3.5 metric tons prohibited, Actual: Vehicles over 3.5 metric tons prohibited\n",
      "Predicted: Speed limit (30km/h), Actual: Speed limit (30km/h)\n",
      "Predicted: Keep right, Actual: Keep right\n",
      "Predicted: Turn right ahead, Actual: Turn right ahead\n",
      "Predicted: Right-of-way at the next intersection, Actual: Right-of-way at the next intersection\n",
      "Predicted: Keep right, Actual: Keep right\n",
      "Predicted: General caution, Actual: General caution\n",
      "Predicted: Priority road, Actual: Priority road\n",
      "Predicted: Road work, Actual: Road work\n",
      "Predicted: Ahead only, Actual: Ahead only\n"
     ]
    }
   ],
   "source": [
    "print_sample_predictions(model, test_loader, device, labels, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the checkpoints & saving torch script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_traffic_sign_model.ckpt'))\n",
    "convert_to_torchscript(model, device, filename='traffic_sign_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
